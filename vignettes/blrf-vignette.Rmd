---
title: "Introduction to BLRF"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to BLRF}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
author: Yujia Lu, Yating Ge, Yuqing Yang
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(BLRF)
```

### 1.1 Introduction to BLRF

* This library combines bag of Little Bootstraps, also known as BLB, to replace the process of ordinary bootstrapping in the standard Random Forest algorithm in order to gain more robustness in prediction for random forest classification as well as regression. 

* Like its name implies, there are a large number of individual decision trees involved in the random forest. The trees work as an ensemble. Each individual tree in the random forest returns a prediction. Finally, the model’s prediction is determined by the most votes of all decision trees. 

* Regarding to ordinary random forest, say we have a training set of size N. Each individual tree randomly takes a random sample of size N with replacement. Through,  ordinary bootstrapping is a powerful tool for approximating quantities, the method’s reliance on repeated resampling make it computationally intensive and ill-suited for extremely large date  sets. Thus we introduce the Bag of Little Bootstraps (BLB) as an alternative approximation. The Bag of Little Bootstraps (BLB) implements a different way to bagging. BLB combines features of the bootstrap and subsampling to form a resampling method well-suited for computations on large data sets while maintaining the favorable statistical properties of the bootstrap (Kleiner et al., 2014).

### 1.2 Internal functions (hidden functions) shipped within this library

| function name   | Description                                                      | Return value      |
|-----------------|------------------------------------------------------------------|-------------------|
|`subsampling()`   | Sampling original data into s subsamples without replacement    | list of subsamples|
|`weights()`     | Calculate weights of each observation in subsample                | matrix of weights |
|`one_tree()`     | Implement random forest once and return a one_tree object        | one_tree object   |
|`tree_implement()` | Build Little Random Forest (LRS) for one subsample          | LRS               |
|`Confusion_one_tree()` | Calculate confusion matrix for each response variable for one tree| confusion_matrix|
|`residual_ci()` | Calculate residual confidence interval for regression random forest| matrix |
|`implement_check_input()` | Checking if all inputs are valid for function `blrf()`| True or False|
|`predict_check_input()` | Checking if all inputs are valid for function `predict.blrf()`| True or False|
|`plot_one_tree()` | Plot one tree with given text object of plot| recordedplot|


### 1.3 Fit little random forests `brlf()`

* The method of `brlf()` function implements little random forests on the trainning dataset, returns a `brlf` object. 

* function `brlf()` : main parameters included in the function

| Parameter       | Definition in BLRF                                          |
|-----------------|-------------------------------------------------------------|
| n               | Size of training data set                                 | 
| $\gamma$(gamma) | The user-defined sizing factor that determinses value of b | 
| s               | Total number of Little Random Forests             | 
| b               | Number of distinct observations in each Little Forest, $b = n^{\gamma}$| 
| r               | Number of trees                    |
| nvar            | Number of variables(ramdonly subset) to subset to build one tree.  | 
| split           | Can be "deviance" or "gini". Default to be "gini" | 
| control          | tree::tree.control | 
| core            | Number of core to used for parallel computing | 

* `brlf` object
  + when implement on random forest classification ,`brlf` object includes:    `Tree_object$Trees`,`Tree_object$fitted_prob`,`Tree_object$fitted_label`,`Tree_object$accuracy_ci`
  
  + when implement on random forest regression ,`brlf` object includes:   
  `Tree_object$Trees`, `Tree_object$fitted`, `Tree_object$residuals`, `Tree_object$residuals_ci`
  
### 1.4 Make predictions `predict()`

  * Random forest could be applied in both solving classification and regression problems. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. We implement methods so that to meet the requirements of different problem solving targets.
  
  
  + Classification：`predict()` is able to return predictions for labels or probabilities; able to return confidence interval for predicted probabilities. `accuracy_mean_ci()` is able to calculate overall average accuracy and the confidence interval of accuracy aggreating all Trees.
  
  + Reregssion：`predict()` is able to return predictions of regression; able to get the percentile confidence intervals for regression predictions.
  

### 1.5 Plot trees `print_trees()`  

To understand trees well,we could apply `print_trees()` function to get more features related to trees bulit in little random forests. This method enables to plot multiple trees with given the list/vector indexes of target trees in the blrf object.
  

### 2.1 Example of Little Random Forest Classification

#### The following section describes how to use the library for random forest classification problem.

* Load data set 'glass' for classification, sample 75% for train set and 25% for test set  
reference : https://www.kaggle.com/uciml/glass/kernels

```{r}
## load train data set and test data set for classification
load("../tinydata/train_glass_sample.Rda")
load("../tinydata/test_glass_sample.Rda")
test_glass_sample_x <- subset(test_glass_sample, select = -c(Type))
test_glass_sample_y <- test_glass_sample[c('Type')]
```


* Run `brlf()` to create little random forest object with multiple attributes; when **core = 1**, indicating without parallel computation; 

```{r, warning=FALSE, results='hide', message=FALSE}
## run brlf() method, core = 1 without parallel computation
cls_blrf <- blrf(Type~., train_glass_sample, gamma=0.82, b = NULL, s=10, r=100, n_var=5,  core = 1)
## attributes for brlf object
cls_blrf$Trees
cls_blrf$fitted_prob
cls_blrf$fitted_label
cls_blrf$accuracy_ci
```

* Run `brlf()` to create little random forest objects with multiple attributes; when **core = 4(or core > 1)**, indicating with parallel computation of 4 cores.  

```{r, warning=FALSE, results='hide',message=FALSE}
## run brlf() method, core = 4 having parallel computation with core = 4
cls_blrf <- blrf(Type~., train_glass_sample, gamma=0.82, b = NULL, s=10, r=200, n_var=5,  core = 4)
## attributes for brlf object
cls_blrf$Trees
cls_blrf$fitted_prob
cls_blrf$fitted_label
cls_blrf$accuracy_ci
```


* Run `print_trees()` to plot multiple trees that bulit by `blrf()`
```{r echo=FALSE, fig.width=8, fig.height= 6}
print_trees(cls_blrf,c(2))
```


* Run `predict()` function to make predictions: **set confidence = F, set probability = F, pretty = T**, return predicted labels for data
```{r}
## make label prediction
y_label <- predict(cls_blrf, test_glass_sample_x, confidence = F, probability = F,pretty = T)
y_label[[1]]
```

* Run `predict()` function to make predictions: **set confidence = F, probability = T, pretty = T**, return predicted labels for data
```{r}
## make label prediction
y_prob <- predict(cls_blrf, test_glass_sample_x, confidence = F, probability = T,pretty = T)
y_prob[1,]
```

* Run `predict()` function to make predictions: **set confidence = T, probability = T, pretty = F**, return predicted labels for data
```{r}
## make label prediction
y_prob_ci <- predict(cls_blrf, test_glass_sample_x, confidence = T, probability = T,pretty = F)
y_prob_ci[1,]
```

* Run `accuracy_mean_ci()` to calculate overall average accuracy and the confidence interval of accuracy aggreating all Trees. Returan mean accuracy as well as confidence interval of accuracy for all y lables.

```{r}
result <- accuracy_mean_ci(cls_blrf, test_glass_sample, lower = 0.025, upper = 0.975)
result
```


### 2.2 Example of Little Random Forest Regression

#### The following section describes how to apply this library for random forest regression  .

* Load data set 'mortality' for regression, sample 75% for train set and 25% for test set
* Mortality data set: the response variable (Mortality) was measured by the deaths per 100,000 population from all causes. Six independent variables were considered: annual precipitation, education, non-white, poverty, NOx and SO2. 

```{r}
load("../tinydata/train_mortality_sample.Rda")
load("../tinydata/test_mortality_sample.Rda")
test_mortality_sample_x <- subset(test_mortality_sample, select = -c(MORTALITY))
test_mortality_sample_y <- test_mortality_sample[c('MORTALITY')]
```

* Run `brlf()` to create little random forest objects with multiple attributes; when **core = 1**, indicating without parallel computation

```{r, warning=FALSE, results='hide',message=FALSE}
## run brlf() method, core = 1 not having parallel computation with core = 4
rg_blrf <- blrf(MORTALITY~., train_mortality_sample, gamma=0.6, b = NULL, s=10, r=100, n_var=5,  core = 1)
## attributes for brlf object
rg_blrf$attrs
rg_blrf$Trees
rg_blrf$fitted
rg_blrf$residuals
rg_blrf$residuals_ci
```


* Run `brlf()` to create little random forest objects with multiple attributes; when **core = 4(or core > 1)**, indicating with parallel computation of 4 cores.

```{r, warning=FALSE, results='hide',message=FALSE}
## run brlf() method, core = 4 having parallel computation with core = 4
rg_blrf <- blrf(MORTALITY~., train_mortality_sample, gamma=0.5, b = NULL, s=10, r=500, n_var=5,  core = 4)
## attributes for brlf object
rg_blrf$attrs
rg_blrf$Trees
rg_blrf$fitted
rg_blrf$residuals
rg_blrf$residuals_ci
```


* Run `predict()` function to make predictions: **set confidence = F, probability = F, pretty = T**, return regression predictions for data

```{r}
y_pred <- predict(rg_blrf, test_mortality_sample_x, confidence = F, probability = F, pretty = T)
y_pred[1]
```


* Run `predict()` function to make predictions: **set confidence = T, probability = F, pretty = F**, return confidence interval for regression predictions for data
```{r}
y_pred_ci <- predict(rg_blrf, test_mortality_sample_x, confidence = T, probability = F, pretty = T)
y_pred_ci$fit[1]
y_pred_ci$ci[1]
```

